Speech is often our first form of communication and expression of emotions. Speech Emotion Recognition is a complex problem, as emotional expression depends on spoken language, dialect, accent, and the cultural background of individuals. The intensity of this emotion can affect our perception and lead us to interpret information inappropriately, with potential applications in various fields such as: Patient monitoring, security, commercial systems, and entertainment. This work performed a Machine Learning task using Deep Learning to infer the intensity of emotions in Portuguese speech, employing Domain Fusion with two distinct databases. To do so, an Autoencoder was created to extract features, and then a supervised model to classify intensities into four classes: (i) Weak; (ii) moderate; (iii) high; and (iv) peak intensity. The results indicate the possibility of inferring intensity, although the final dataset is limited, even when combining two datasets. Two experimental scenarios were carried out, with analogous architectures, varying only the quantity of representative features used as input for the models. Additionally, observing the performance metrics on both experiments, it was possible to note the recurrence of the same class (strong) with the lowest variation while the most distant classes (weak and peak) had the best performance, which raises questions for further studies.