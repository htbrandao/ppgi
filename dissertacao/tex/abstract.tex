Speech is often our first form of communication and expression of emotions. Speech Emotion Recognition (SER) is a complex problem, as emotional expression depends on spoken language, dialect, accent, and the cultural background of individuals. The intensity of this emotion can affect our perception and lead us to interpret information inappropriately, with potential applications in various fields such as: Patient monitoring, security, commercial systems, and entertainment. This work performed a Machine Learning (ML) task using both Machine Learning and Deep Learning (DL) to infer the intensity of emotions in Portuguese speech, employing Domain Fusion with two distinct databases. To do so, an Autoencoder was created to extract features, and then we trained a supervised model to classify intensities into four classes: (i) Weak; (ii) moderate; (iii) high; and (iv) peak intensity. The results indicate the possibility of inferring intensity, although the dataset is limited, even when combining two datasets. Two experimental scenarios were carried out, with analogous architectures, varying only the quantity of representative features used as input for the models. Additionally, observing the performance metrics, it was possible to note the recurrence of the same class (strong) with the lowest variation between both experiments, which raises questions for further studies.