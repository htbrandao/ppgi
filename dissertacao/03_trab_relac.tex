\chapter{Trabalhos Relacionados}\label{Cap:Trabalhos Relacionados}

Neste capítulo serão discutidos trabalhos relacionados e utilizados como referencial teórico por este material. Trataremos sobre suas propostas, características, metodologias, possíveis limitações e possibilidades de melhoria. Posteriormente, será feita uma análise comparativa com o trabalho proposto no Capítulo \ref{Cap:Projeto de Pesquisa}, evidenciando pontos em comum e possíveis divergências e, por fim, como este trabalho se propõe a colaborar com a produção científica e o estado da arte na área de \textit{SER}. Os trabalhos infracitados se encontram dispostos em ordem cronológica de publicação vide Tabela \ref{table:discussao}.\\

% ------------------------------------------------------------------------------------------

No âmbito das Ciências da Computação, o processamento de voz é uma área de pesquisa ativa, com publicações datando desde o final do século XX, podemos citar \cite{12.27} no ano de 1991 e \cite{12.28} em 1995.

% ------------------------------------------------------------------------------------------

Donn Morrison, Ruili Wang e Liyanage C. De Silva, em \textit{Ensemble methods for spoken emotion recognition in call-centres} \cite{32.32}, dada a ubuiquidade de sistems automatizados, lembram da necessidade de aprimorar a naturalidade das interações humano-computador e da importância de interpretar com precisão informações emocionais. Além de um \textit{dataset} tradicional (ESMBS), utilizou uma base de dados fornecida por uma companhia elétrica, composto por ligações de clientes para o \textit{call-center} da empresa, o que significa que não consistia de um \textit{dataset} simulado ou seminatural, e sim de uma massa de dados com amostras naturais e espontâneas. Afirma que um modelo de \textit{SER} pode colaborar com o atendimento aos clientes em chamadas de acordo com a urgência percebida. O que corrobora com este trabalho na importância, não só do diagnõstico da emoção, como também da sua intensidade. Caso seja detectada determinada emoção, o sistema transferiria a ligação para um atendente humano fornecer assistência. Entretanto, mesmo com a combinação de duas massas de dados, o dataset final contava com pouco mais de mil amostras (1100), sendo 93\% pertencente a classe neutra. Quanto aos algoritmos, foram utilzados dez modelos tradicionais (\textit{SVM radial basis}, \textit{KNN}, \textit{MLP}, \textit{RF}, \textit{K*}, \textit{NAIVE BAYES}, \textit{SVM polinomial}, \textit{Decision Tree e Random Tree}), combinados em forma de ensemble, sem pesos individuais, para realizar a predição, alcançando acurácia média (72,18\%) ligeiramente superior aos modelos individuais.

Mayank Bhargava e Tim Polzehl, em \textit{Improving Automatic Emotion Recognition from speech using Rhythm and Temporal feature} \cite{11}, se propõem a melhorar o desempenho em tarefas de \textit{SER} incorporando mais features aos dados de entrada. Aponta que pesquisas à época dependiam fortemente de \textit{MFCCs}, tonalidade e da amplitude. Dicidiu utilizar um algoritmo (\textit{Voice Activity Detection}, \textit{VAD}) para separar, a nível de amostra, os momentos com linguagem presente, e performar a extração de características separadamente. Validou sua abordagem com dois modelos, um \textit{DNN} e um \textit{SVM}, ao longo de sete classes. Apesar dos modelos, o escopo do trabalho seria o de relacionar a melhora no desempenho dos modelos com a adição ou combinação de mais features, entretanto, observou que os melhores resultados de índice de acerto foram obtidos em dois cenários: (1) 74.02\% utilizando \textit{MFCCs} e características rítmicas, embora o resultado utilizando apenas features rítmicas fosse de apenas 34.6\%; e (2) 71.93\% utilizando apenas \textit{MFCCs}. Sobre as características rítmicas, ressaltou que sua baixa contribuição dá-se possivelmente em virtude da baixa dimensionalidade, uma vez que é mais de dez vezes menor que a dos MFCCs. Os resultados de \cite{11} foram, de certa forma, contrários a sua hipótese, e reasseguraram a eficiência de utilizar features espectrográficas para tarefas de SER.

Xiaoming Zhao e Shiqing Zhang, em\textit{ Spoken emotion recognition via locality-constrained kernel sparse representation }\cite{32.31}, propõem um método para classificação de emoções denominado \textit{locality-constrained kernel sparse representation-based classification} (\textit{LC-KSRC}), capaz de aprender com representações de coeficientes mais esparsas, utilizando features prosódicas, espectrais e de qualidade da voz. Uma vez que a localidade dos dados tem tem sido amplamente utilizada em muitos problemas de reconhecimento de padrões como \textit{clustering}, \textit{dimensionality reduction} e classificação de imagens, \cite{32.31} combina as técnicas \textit{Sparse representation-based classification} (\textit{SRC}), \textit{Kernel sparse representation-based classification} (\textit{KRSC}) e \textit{Locality-constrained linear coding} (\textit{LLC}) para formar \textit{LC-KSRC} e tentar compor sobre o problema de perda de captura da estrutura dados locais de \textit{KRSC}. Testado em quatro bases de dados distitnas, o modelo proposto apresenteu acurária superior a todos os outros seis modelos com o qual foi comparado. Xiaoming Zhao e Shiqing Zhang apontam para a o desafio de produzir soluções de SER que atuem em tempo real.

Sonia Xylina Mashal e Kavita Asnani, em\textit{ Emotion Intensity Detection for Social Media Data }\cite{14}, observam que embora sejam desenvolvidos trabalhos na área de reconhecimento de emoções, essa classificação é um resultado de mais rudimentar, e o próximo passo seria determinar a intensidade dessa emoção. Portanto, investigaram um classificador para determinar a intensidade das emoções em textos, utilizando dados obtidos a partir de uma base de dados formada por textos do Twitter. As amostras eram compostas por textos curtos (tweets) e um valor entre 0 e 1 relativo a intensidade da emoção sentida pelo autor.

Zhang, S. et al., em \textit{Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching} \cite{32.25}, aponta o distanciamento entre as emoções, subjetivas, e as features de baixo nível. Observa o bom desempenho de \textit{CNNs} em tarefas de reconhecimento de imagem e detecção de objeto e explora como utilizar essas redes para tentar encurtar essa distância. De maneira análoga ao que é feito em tarefas que envolvem imagens, extraindo os três canais correspondentes as cores vermelho, verde e azul (red, green e blue, respectivamente, formando a sigla \textit{RGB}), este, a partir do \textit{Mel Spectrogram} da amostra, extrai três canais: Estático (\textit{static}), delta, e delta delta. Para realizar \textit{feature extraction}, utiliza a \textit{CNN} \textit{AlexNET}, uma rede neural pré-treinada para reconhecimento de imagens, para tentar aprender características de alto nível a respeito das vocalizações. Propõe então uma \textit{Discriminant Temporal Pyramid Matching} (\textit{DTPM}), inspirado na \textit{Spatial Pyramid Matching} (\textit{SPM}), para agrupar as features aprendidas em uma unidade maior e aplicando um modelo \textit{SVM} linear na nesse resultado. Embora tenha utilizado mais de um dataset em sua tarefa, \cite{32.25} aponta a quantidade limitada de amostras disponíveis para treinamento e afirma que o modelo AlexNET apresenta um bom desempenho na tarefa de feature extracion.

Sefik Emre Eskimez, Zhiyao Duan e Wendi Heinzelman, em \textit{UNSUPERVISED LEARNING APPROACH TO FEATURE ANALYSIS FOR AUTOMATIC SPEECH EMOTION RECOGNITION} \cite{34}, propõem a utilização de modelos não supervisionados do tipo autoencoder, separadamente, para tentar remediar a escassez de dados para tarefas de \textit{SER}, questionando a viabilidade de aprendender \textit{features} de \textit{datasets} de outros domínios de voz e utilizá-los para treinar modelos de classificação de emoções. Os modelos não supervisionados incluem\textit{ Denoising Autoencoder} (\textit{DAE}), \textit{Variational Autoencoder} (\textit{VAE}), \textit{Adversarial Autoencoder} (\textit{AAE}) e \textit{Adversarial Variational Baye}s (\textit{AVB}) com adição de ruído. Utilizou um \textit{SVM} e \textit{CNN} como modelos de base para avaliar o desempenho dos \textit{autoencoders}. Os modelos receberam\textit{ Mel Spectrograms} para efetuar a classificação das emoções, e \cite{34} chegou a conclusão de que os modelos inferenciais (\textit{VAE}, \textit{AAE} e \textit{AVB}) obtiveram um desempenho superior tarefa de aprendizado não supervisionado de aprendizado de features.

Yuanchao Li, Tianyu Zhao e Tatsuya Kawahara, em \textit{Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning} \cite{32.95}, propõem um modelo \textit{multitask} utilizando uma camada de autoatenção (\textit{self attention layer}) para classificar tanto a emoção quanto o sexo da pessoa, apesar das dificuldades em virtude da variabilidade em dados de fala e emoção. Utiliza os espectrogramas como \textit{input} para um modelo composto por uma CNN\textit{ }que alimenta uma \textit{BLSTM} seguida pela camada de autoatenção para agregar informações da camada anterior ao longo do tempo. Utilizando a base de dados IEMOCAP, afirmou ter superado (à época, 2019) os melhores resultados de classificação relativos ao dataset.

M. S. Akhtar et al., em \textit{All-in-One: Emotion, Sentiment and Intensity Prediction using a Multi-task Ensemble Framework} \cite{28}, propõe um \textit{ensemble} de modelos para a multitarefa de classificar emoção e intensidade em texto. Utilizando 3 modelos para feature extraction (\textit{\textit{CNN, \textit{LSTM} e GRU}}) e processamento de linguagem natural (\textit{NLP}) para adicionar mais uma dimensão ao vetor de entrada, observa que modelos multitarefa constumam ter desempenhos superiores em matéria de generalização. As bases de dados utilizadas apresentavam as emoções como classe, e a intensidade foi calculada a partir da técnica de Vader\footnote{Disponível em \url{https://ojs.aaai.org/index.php/ICWSM/article/view/14550}}. Akhtar avaliou tanto o desempenho dos modelos individuais quanto combinações (\textit{ensembles}) para concluir que os \textit{ensembles} propostos apresentaram desempenho superior aos modelos individuais.

Zhu et al., em\textit{ Controlling Emotion Strength with Relative Attribute for End-to-End Speech Synthesis} \cite{63}, pretende melhorar a performance da síntese de uma vocalização emocional oriunda de um mecanismo de conversão de texto para fala (\textit{speech to text}, \textit{STT}). Focando apenas no controle sutil da intensidade da emoção, simplificando o controle da emoção e da intensidade através de um vetor e um escalar, respectivamente, para que consgiga modular com facilidade a itensidade de uma amostra. A base de dados utilizada é composta por várias sentenças que são repetidas tendo sua classe (emoção) alterada. Embora não possua labels para a intensidade, \cite{63} busca calcular uma função de ranqueamento de atributo (\textit{attribute ranking function}) entre pares formados por classes distintas de uma mesma amostra e utilizar esse atributo em combinação com uma amostra de uma classe para produzir a amostra pertencente outra classe, de acordo com o atributo. Uma vez calculada a \textit{ranking function}, um algoritmo não supervisionado de redução de dimensionalidade (\textit{PCA}) foi aplicado numa amostra da base de dados, onde foi observada a coerência entre as classes de dados dados de um mesmo conjunto (\textit{cluster}).

Aggelina Chatziagapi et al., em \textit{Data Augmentation using GANs for Speech Emotion Recognition }\cite{32.89}, propõem um modelo de \textit{GAN} para atacar o problema de conjuntos de dados desbalanceados em tarefas de \textit{SER}. Fato que podemos observar em \cite{32.32} e soma-se a um ponto descrito nesse trabalho, sobre a discrepância em volume de dados para tarefas de \textit{SER} com realação a outras tarefas, como reconhecimento de imagens, e mais ainda no contexto de intensidade das emoções. Busca produzir novas amostras de spectrogramas para classes menos representadas. Embora não seja propriamente uma proposta para reconhecimento de emoções ou de intensidade das emoções, \cite{32.89} utiliza duas bases de dados, IEMOCAP e FEEL-25k, para validar o resultado do seu modelo. Uma vez que os spectrogramas gerados artificialmente são incluídos ao conjunto do dataset, calcula a distribuição das classes, e então passa a retirar amostras aleatoriamente, observando que as porcentagens das classes se mantém equilibrada ao longo da remoções.

Campos e Moutinho, em DEEP: Uma arquitetura para reconhecer emoção com base no espectro sonoro da voz de falantes da língua portuguesa \cite{12}, se propõem a criar um modelo mais robusto, com uma implementação híbrida. Observaram que a utilização de múltiplas redes neurais de maneira sequencial pode ocasionar a propagação de erros entre os modelos. Assim, seu modelo é composto por vários modelos especialistas, que combinam redes neurais convolucionais e redes neurais profundas, treinados de forma supervisionada. Campos e Coutinho treinaram um modelo especialita para cada uma das sete emoções presentes no dataset VERBO. Além de reiterar a usabilidade da base de dados, tiveram ganhos em sua taxa média de acerto acima de 10\% quando comparando sua implementação com uma CNN simples.

Neelakshi Josh, em \textit{Brazilian Portuguese emotional speech corpus analysis} \cite{20}, se propôs a explorar diferentes features de um banco de dados em poruguês brasileiro. Procurou explorar características espectrais, prosódicas e temporais numa tarefa de SER, afirmando que a utilização conjunta dessas características podem melhorar o seu índice de acerto. Totalizando 38 \textit{features} em um vetor, treinou quatro algoritmos, utilizando apenas a abordagem supervisionada: \textit{SVM}, \textit{MLP},\textit{RF} e \textit{KNN}. Utilizou a base de dados VERBO, também utilizada neste trabalho, colaborando com a evidência de que apesar de ser o primeiro dataset para SER em português brasileiro, este pode ser utilizado para trabalhos de \textit{SER} e que suas amostras possuem características suficientes para desenvolver tarefas de \textit{ML}. Embora tenha utilizado quatro modelos distintos, Neelakshi não se aprofundou em demais arquiteturas de \textit{Deep Learning}.

Abbaschian, Sierra-Sosa e Elmaghraby, em \textit{Deep Learning Techniques for Speech Emotion Recognition from Databases to Models} \cite{32}, revisam publicações que envolvem trabalhos de \textit{Deep Learning} para \textit{SER}, bem como as bases de dados utilizadas. Realiza um comparativo entre onze bases de dados amplamente utilizadas em trabalhos dessa natureza, dispostas em três categorias: Simuladas, seminaturais e naturais. Realizam um cruzamento entre 25 databases e trabalhos que as utilizam, organizando essa relação ao longo do tempo, disposto num eixo horizontal que compreende os anos entre 2005 e 2020. Nessa relação podemos perceber a predominância de certos entes em determinados períodos do tempo, tanto com relação a datasets quanto a modelos. Quanto as bases de dados, a partir de 2018 percebemos a predominância do IEMOCAP, sendo utilizado o dobro de vezes em relação ao EMO-DB, segundo mais utilizado. Quanto aos modelos, percebemos a presença constante de \textit{CNNs} a partir de 2016 e de variações de \textit{Autoencoders} a partir de 2018, ambos permanecendo presentes até 2020, ano final da cobertura da pesquisa.

% \cite{18}-2022
Kun Zhou et al., em \textit{Emotion Intensity and its Control for Emotional Voice Conversion }\cite{18}, comenta a parca presença de estudos relativos a intensidade da emoção, uma vez que têm um grande potencial para a conversão de voz. O trabalho consistem em conseguir controlar a intensidade da emoção em um modelo de sequência para sequência (\textit{seq2seq}) que converte texto para voz. De maneira análoga a \cite{63}, Zhou também decide encontrar uma \textit{attribute ranking function} para operar com o dado de entrada e alterar a intensidade da emoção da maneira desejada. Kun \cite{18} e Zhu \cite{63} diferem na sua arquitetura, enquanto \cite{63} utiliza um \textit{autoencoder} que recebe o escalar relativo a intensidade após a fase de \textit{encoding}, \cite{18} utiliza três modelos com funções disntantas: (1) Extrair as features do \textit{input}; (2) Criar adicionar a emoção desejada para a saída; e (3) Adicionar a alteração na intensidade.

% ==========================================================================================
\section{Discussão dos Trabalhos Relacionados}

Conseguimos observar nos trabalhos supracitados que dificuldades relativas à massa de dados não é um problema exclusivo de trabalhos em SER, uma vez que os trabalhos que lidam dados textuais reiteram essa queixa, sejam os que lidam puramente com textos ou os que realizam tarefas de \textit{text-to-speach} com emoção.

Ainda no ano de de 2007, \cite{32.32} lembrou da ubiquidade de sistemas automatizados, que se mostra ainda mais presente contemporaneamente. Ao efetuarmos um comparativo (Tabela \ref{table:discussao}) entre trabalhos com tarefas de reconhecimento de emoções na fala, vemos que apesar de \cite{14} afirmar que a atividade de classificaçar emoções poderia ter um papel adicional enquanto catalisador para trabalhos envolvendo a intensidade da emoção, no entantdo, a incidência de trabalhos nessa área específica parece baixa.

A subjetividade das emoções demanda cuidado e atuação profissional na composição de bancos de dados para esse tipo de tarefa. Ao reduzir o escopo da pesquisa para trabalhos com dados em português, a literatura se torna ainda mais escassa, haja vista a publicação do VERBO apenas no ano de 2018.

Características espectrais aparentam ser uma feature importante para os modelos, carregadando bastante informacional sobre a amostra. Pudemos observar em \cite{11} que a ausência de \textit{MFCCs} acarretou uma queda superior a 50\% na performance da classificação, enquanto \cite{34} os utilizou como \textit{target} para redes generativas enquanto buscava aumentar a quantidade de amostrass disponíveis para treinamento de modelos.

Outro ponto a ser observado é que embora existam trabalhos utilizando mais de um \textit{dataset}, essa prática ainda não parece estar totalmente difundida entre as publicações de \textit{SER}, uma vez que os escopos e naturezas (simulada, seminatural ou natural) das bases de dados costumam ser bastante distintas.

Analisando as informações expostas na Tabela \ref{table:discussao}, podemos notar que estre trabalho se aproxima dos demais por utilizar features espectrais e envolver uma abordagem supervisionada. Também tem em comum o fato de utilzar técnicas consolidadas ("clássicas") de \textit{ML}, ao mesmo tempo que investiga arquiteturas de \textit{DL}, como DNNs, \textit{CNNs} e \textit{Autoencoders}. Entretanto, este trabalho começa a se distanciar dos demais quando resolver trabalhar com áudios para a língua portuguesa. Além disso, a abordagem não supervisionadas também será utilizada, o que não é tão comum em uma mesma tarefa de \textit{SER}. Outro ponto de inovação dá-se uma vez que os trabalhos que encontramos utilzando o dataset VERBO têm apenas utilizaram apenas aprendizagem supervisionado e não lidam com intensidade. Ademais, não foram encontrados trabalhos utilizando o dataset VIVAE (2020), possivelmente tornando este trabalho pioneiro.

% \clearpage

% -------------------------------------------------------------------------------------------

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% gerada em https://www.tablesgenerator.com/
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\begin{landscape}
\begin{table}[]
\centering
\label{table:discussao}
\caption{Comparativo entre BRAVO e literatura correlata}
\begin{tabular}{|
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}l |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |}
\hline
\cellcolor[HTML]{FFFFFF} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{3}{c|}{\cellcolor[HTML]{FFFFFF}Features} & \multicolumn{2}{c|}{\cellcolor[HTML]{FFFFFF}Abordagem} & \multicolumn{2}{c|}{\cellcolor[HTML]{FFFFFF}Arquitetura} & \multicolumn{2}{c|}{\cellcolor[HTML]{FFFFFF}Português} & \multicolumn{2}{c|}{\cellcolor[HTML]{FFFFFF}Resultado} \\ \cline{3-13} 
\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}ANO} & \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}Referência}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Cromática} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Espectral} & Prosódica & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Superv.} & Não superv.& \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}ML} & DL & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Não} & Sim & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Emoção} & Intensidade \\ \hline
% 1995 & {\cite{12.28}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} &  \\ \hline
2007 & {\cite{32.32}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2013 & {\cite{11}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} &  \\ \hline
2015 & {\cite{32.31}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2018 & {\cite{32.25}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2018 & {\cite{34}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2019 & {\cite{32.95}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2020 & {\cite{12}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
2021 & {\cite{21}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}} & X & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}X} &  \\ \hline
\textit{\textbf{2023}} & \textit{\textbf{BRAVO}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{X}}} & \textit{\textbf{}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{X}}} & \textit{\textbf{X}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{X}}} & \textit{\textbf{X}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{}}} & \textit{\textbf{X}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}\textit{\textbf{}}} & \textit{\textbf{X}} \\ \hline
\end{tabular}
\end{table}
\end{landscape}
